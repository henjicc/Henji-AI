# 创建聊天对话请求

向指定端点发送 POST 请求，根据指定的聊天对话生成模型回复。

**请求 URL**
```
POST https://api.ppinfra.com/openai/v1/chat/completions
```

## 请求头

| 参数名 | 类型 | 是否必需 | 描述 |
| :--- | :--- | :--- | :--- |
| `Content-Type` | string | 是 | 枚举值: `application/json` |
| `Authorization` | string | 是 | Bearer 身份验证格式，例如：`Bearer {{API 密钥}}`。 |

## 请求体

| 参数名 | 类型 | 是否必需 | 描述 |
| :--- | :--- | :--- | :--- |
| `model` | string | 是 | 对应的模型名称。可用模型名称请参考：[模型列表](https://ppio.com/model-api/product/llm-api) |
| `messages` | object[] | 是 | 截至目前组成对话的消息列表。 |
| `max_tokens` | integer | 是 | 在生成对话时可产生的最大 tokens 数。如果您的提示（之前的消息）中的 tokens 数量加上 max_tokens 超过了模型的上下文长度，则行为取决于 context_length_exceeded_behavior。默认情况下，max_tokens 会被调整以适应上下文窗口，而不是返回错误。 |
| `stream` | boolean \| null | 否 | 是否使用流式传输。默认为 false，如果设置了，tokens 将以 data-only server-sent events（SSE）发送，并以 data: [DONE] 消息终止流。 |
| `stream_options` | object | 否 | 流式回复选项。仅当 stream 设置为 true 时设置。 |
| `n` | integer \| null | 否 | 每个提示生成多少个对话。默认值为 1。注意：由于此参数会生成多个对话，因此可能会快速消耗您的 tokens 计费额度。请谨慎使用，并确保为 max_tokens 和 stop 设置了合理的值。所需范围：1 < x < 128 |
| `seed` | integer \| null | 否 | 如果指定，我们的系统将尽最大努力进行确定性采样，以便相同的 seed 和参数的重复请求应返回相同的结果。 |
| `frequency_penalty` | number \| null | 否 | 默认值为 0，正值会根据新 tokens 在当前文本中的出现频率对其进行惩罚，从而降低模型重复相同内容的可能性。如果目的是仅仅减少重复样本，合理的值大约在 0.1 到 1 之间。如果目的是强烈抑制重复，可以将系数提高到 2，但这可能会明显降低样本质量。负值可以用来增加重复的可能性。另见 presence_penalty，用于以固定速率惩罚至少出现一次的 tokens。所需范围：-2 < x < 2 |
| `presence_penalty` | number \| null | 否 | 默认值为 0，正值会根据新 tokens 是否出现在当前文本中对其进行惩罚，从而增加模型谈论新话题的可能性。如果目的是稍微减少重复样本，合理的值大约在 0.1 到 1 之间。如果目的是强烈抑制重复，可以将系数提高到 2，但这可能会显著降低样本质量。负值可以用来增加重复的可能性。另见 frequency_penalty，用于根据 tokens 出现的频率按递增速率进行惩罚。所需范围：-2 < x < 2 |
| `repetition_penalty` | number \| null | 否 | 对重复的 tokens 应用惩罚，以抑制或鼓励重复。值为 1.0 表示没有惩罚，允许自由重复。大于 1.0 的值会惩罚重复，降低重复 tokens 的可能性。介于 0.0 和 1.0 之间的值会奖励重复，增加重复 tokens 的机会。为了达到良好的平衡，通常建议使用 1.2。请注意，在仅解码器模型中，惩罚会同时应用于生成的输出和提示。所需范围：0 < x < 2 |
| `stop` | string \| null | 否 | 最多 4 个序列，API 将停止生成更多 tokens。返回的文本包含停止序列。 |
| `temperature` | number \| null | 否 | 对话中的随机性程度，默认值为 1，介于 0 和 2 之间。较高的值（如 0.8）会使输出更加随机，而较低的值（如 0.2），会使输出更集中且确定性更强。我们通常建议只调整此项或 top_p，而不是同时调整两者。所需范围：0 < x < 2 |
| `top_p` | number \| null | 否 | 作为 temperature 的替代方法，称为 nucleus sampling，模型会考虑具有 top_p 概率质量的 tokens 的结果。因此，0.1 意味着只考虑构成前 10% 概率质量的 tokens。我们通常建议只调整此项或 temperature，而不是同时调整两者。所需范围：0 < x ≤ 1 |
| `top_k` | integer \| null | 否 | Top-k 采样是另一种采样方法，在这种方法中，k 个最可能的下一个 tokens 会被筛选出来，并且概率质量仅在这 k 个 tokens 之间重新分配。k 的值控制了在每一步生成文本时，下一个 tokens 的候选数量。所需范围：1 < x < 128 |
| `min_p` | number \| null | 否 | 表示一个 tokens 被考虑的最小概率的浮动值，相对于最有可能的 tokens 的概率。所需范围：0 ≤ x ≤ 1 |
| `logit_bias` | map[string, integer] | 否 | 默认为 null。修改指定 tokens 在对话中出现的可能性。接受一个 JSON 对象，将 tokens 映射到一个从 -100 到 100 的关联偏差值。 |
| `logprobs` | boolean \| null | 否 | 是否返回输出 tokens 的对数概率。如果为 true，则返回消息内容中每个输出 tokens 的对数概率。 |
| `top_logprobs` | integer \| null | 否 | 一个介于 0 到 20 之间的整数，指定在每个 tokens 位置返回的最可能的 tokens 数量，每个 tokens 都有一个关联的对数概率。如果使用此参数，必须将 logprobs 设置为 true。所需范围：0 ≤ x ≤ 20 |
| `tools` | object[] \| null | 否 | 模型可以调用的工具列表。目前仅支持函数作为工具。使用此参数提供模型可能生成 JSON 输入的函数列表。在[工具调用（Function Calling）](/docs/model/llm-function-calling)中了解更多关于函数调用的信息。 |
| `response_format` | object \| null | 否 | 允许强制模型产生特定的输出格式。设置为 `{ "type": "json_schema", "json_schema": {...} }` 启用结构化输出，确保模型匹配您提供的 JSON schema。设置为 `{ "type": "json_object" }` 启用较旧的 JSON 模式，确保模型生成的消息是有效的 JSON。对于支持的模型，推荐使用 `json_schema`。 |
| `separate_reasoning` | boolean \| null | 否 | 控制是否将 “思考内容” 单独放入 “reasoning_content” 字段中。默认为 false，即不单独返回 “思考内容”。目前支持的模型：deepseek/deepseek-r1-turbo |
| `enable_thinking` | boolean \| null | 否 | 控制是否开启思考模式。默认值: false。目前支持的模型：zai-org/glm-4.5, deepseek/deepseek-v3.1, deepseek/deepseek-v3.1-terminus, deepseek/deepseek-v3.2-exp |

### messages 对象

| 参数名 | 类型 | 是否必需 | 描述 |
| :--- | :--- | :--- | :--- |
| `content` | string \| object[] \| null | 是 | 消息的内容。所有消息都需要 content，对于包含函数调用的 assistant 消息，content 可以为 null。您可以根据不同的模态使用以下参数。 |
| `role` | string | 是 | 消息作者的角色。可以是 system、user、assistant 之一。枚举值: `system`, `user`, `assistant` |
| `name` | string \| null | 否 | 此消息的作者名称。可以包含小写字母 a-z、大写字母 A-Z、数字 0-9 和下划线，最大长度为 64 个字符。 |

#### content 对象 (当为 object[] 时)

内容部分的数组，详细字段如下：

| 参数名 | 类型 | 是否必需 | 描述 |
| :--- | :--- | :--- | :--- |
| `type` | string | 是 | 内容部分的类型。 |
| `text` | string | 是 | 文本内容。 |
| `image_url` | string | 是 | 图像的 URL 或 base64 编码的图像数据。仅视觉语言模型可使用。 |
| `video_url` | string | 是 | 视频的 URL。仅支持视频的模型可使用。 |
| `modalities` | string[] | 否 | 设置模型输出的模态。当前支持两种：`["text"]`、`["text","audio"]`。如果传入 `["text"]`，模型只返回文字内容。如果传入 `["text","audio"]`，模型会同时返回文字内容和音频内容。默认为 `["text"]`。 |
| `input_audio` | object | 是 | 音频输入内容。仅支持音频的模型可使用。 |

##### input_audio 对象

| 参数名 | 类型 | 是否必需 | 描述 |
| :--- | :--- | :--- | :--- |
| `data` | string | 是 | 音频的 URL 或者 base64 编码。 |
| `format` | string | 是 | 音频的格式。如 `wav`, `mp3`。 |

### stream_options 对象

| 参数名 | 类型 | 是否必需 | 描述 |
| :--- | :--- | :--- | :--- |
| `include_usage` | boolean | 否 | 如果设置，将在 data: [DONE] 消息之前流式传输一个额外的 chunk。此 chunk 中的 usage 字段显示整个请求的令牌使用统计信息，而 choices 字段始终为空数组。所有其他 chunk 也将包含一个 usage 字段，但值为 null。 |

### tools 对象 (数组元素)

| 参数名 | 类型 | 是否必需 | 描述 |
| :--- | :--- | :--- | :--- |
| `type` | string | 是 | 工具的类型。支持的类型：`function` |
| `function` | object | 是 | 函数定义。 |

#### function 对象

| 参数名 | 类型 | 是否必需 | 描述 |
| :--- | :--- | :--- | :--- |
| `name` | string | 是 | 要调用的函数名称。必须是 a-z、A-Z、0-9 或包含下划线和破折号，最大长度为 64 个字符。 |
| `description` | string \| null | 否 | 函数功能的描述，模型使用此信息来选择何时以及如何调用函数。 |
| `parameters` | object \| null | 否 | 函数接受的参数，以 JSON Schema 对象的形式描述。有关格式的文档，请参阅 [JSON Schema 参考](https://json-schema.org/understanding-json-schema/)。 |
| `strict` | boolean | 否 | 是否在生成函数调用时启用严格遵守 schema。如果设置为 true，模型将完全按照 parameters 字段中定义的 schema 进行操作。默认值: false |

### response_format 对象

| 参数名 | 类型 | 是否必需 | 描述 |
| :--- | :--- | :--- | :--- |
| `type` | string | 是 | 枚举值：`text`, `json_object`, `json_schema`。默认值: `"text"` |
| `json_schema` | object \| null | 否 | JSON Schema 响应格式。用于生成结构化的 JSON 响应。仅在 `type` 设置为 `json_schema` 时支持，且当 `type` 设置为 `json_schema` 时为必需项。在[结构化输出](/docs/model/llm-structured-outputs)中了解更多信息。 |

#### json_schema 对象

| 参数名 | 类型 | 是否必需 | 描述 |
| :--- | :--- | :--- | :--- |
| `name` | string | 是 | 响应格式的名称。必须是 a-z、A-Z、0-9 或包含下划线和破折号，最大长度为 64 个字符。 |
| `description` | string \| null | 否 | 响应格式用途的描述，模型使用此信息来确定如何以该格式响应。 |
| `schema` | object \| null | 否 | 响应格式的 schema，以 JSON Schema 对象的形式描述。在[这里](https://json-schema.org/specification)学习如何构建 JSON schema。支持的类型：`string`, `number`, `integer`, `boolean`, `array`, `object`, `enum`, `anyOf`。 |
| `strict` | boolean | 否 | 是否在生成输出时启用严格的 schema 遵守。如果设置为 true，模型将始终遵循 schema 字段中定义的确切 schema。当 strict 为 true 时，仅支持 JSON Schema 的子集。如果您通过提供 `strict: true` 启用结构化输出并使用不支持的 JSON Schema 调用 API，您将收到错误。默认值: false |

## 响应参数

| 参数名 | 类型 | 是否必需 | 描述 |
| :--- | :--- | :--- | :--- |
| `choices` | array | 是 | 生成的对话选择列表。 |
| `created` | integer | 是 | 响应生成的 Unix 时间戳（以秒为单位）。 |
| `id` | string | 是 | 响应的唯一标识符。 |
| `model` | string | 是 | 用于对话的模型。 |
| `object` | string | 是 | 对象类型，始终为 `chat.completion`。 |
| `usage` | object | 是 | 使用统计。对于流式回复，usage 字段被包含在返回的最后一个回复块中。 |

### choices 对象 (数组元素)

| 参数名 | 类型 | 是否必需 | 描述 |
| :--- | :--- | :--- | :--- |
| `finish_reason` | string | 是 | 模型停止生成 tokens 的原因。如果模型遇到自然停止点或提供的停止序列，则为 ‘stop’；如果请求中指定的最大 tokens 数量已达到，则为 ‘length’。枚举值: `stop`, `length` |
| `index` | integer | 是 | 对话选择的索引。 |
| `message` | object | 是 | 生成的消息对象。 |

#### message 对象

| 参数名 | 类型 | 是否必需 | 描述 |
| :--- | :--- | :--- | :--- |
| `role` | string | 是 | 消息作者的角色。枚举值: `system`, `user`, `assistant` |
| `content` | string \| null | 是 | 消息的内容。 |
| `reasoning_content` | string \| null | 否 | 思考内容。目前支持的模型：deepseek/deepseek-r1-turbo |

### usage 对象

| 参数名 | 类型 | 是否必需 | 描述 |
| :--- | :--- | :--- | :--- |
| `completion_tokens` | integer | 是 | 对话生成的 tokens 数。 |
| `prompt_tokens` | integer | 是 | prompt 中的 tokens 数。 |
| `total_tokens` | integer | 是 | 请求中使用的总 tokens 数（prompt + completion）。 |

## 请求示例

```bash
curl --request POST \
  --url https://api.ppinfra.com/openai/v1/chat/completions \
  --header 'Authorization: <authorization>' \
  --header 'Content-Type: application/json' \
  --data '
{
  "model": "<string>",
  "messages": [
    {
      "content": {
        "type": "<string>",
        "text": "<string>",
        "image_url": "<string>",
        "video_url": "<string>",
        "modalities": [
          "<string>"
        ],
        "input_audio": {
          "data": "<string>",
          "format": "<string>"
        }
      },
      "role": "<string>",
      "name": {}
    }
  ],
  "max_tokens": 123,
  "stream": {},
  "stream_options": {
    "include_usage": true
  },
  "n": {},
  "seed": {},
  "frequency_penalty": {},
  "presence_penalty": {},
  "repetition_penalty": {},
  "stop": {},
  "temperature": {},
  "top_p": {},
  "top_k": {},
  "min_p": {},
  "logit_bias": {},
  "logprobs": {},
  "top_logprobs": {},
  "tools": {
    "type": "<string>",
    "function": {
      "name": "<string>",
      "description": {},
      "parameters": {},
      "strict": true
    }
  },
  "response_format": {
    "type": "<string>",
    "json_schema": {
      "name": "<string>",
      "description": {},
      "schema": {},
      "strict": true
    }
  },
  "separate_reasoning": {},
  "enable_thinking": {}
}
'
```

## 响应示例

```json
{
  "choices": [
    {
      "finish_reason": "<string>",
      "index": 123,
      "message": {
        "role": "<string>",
        "content": {},
        "reasoning_content": {}
      }
    }
  ],
  "created": 123,
  "id": "<string>",
  "model": "<string>",
  "object": "<string>",
  "usage": {
    "completion_tokens": 123,
    "prompt_tokens": 123,
    "total_tokens": 123
  }
}
```